{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import itertools\n",
    "import numpy as np                      # numpy normal para preparar datos\n",
    "import jax\n",
    "import jax.numpy as jnp                 # jax numpy para el modelo\n",
    "from jax import random, grad, jit, vmap, hessian\n",
    "from jax.example_libraries import optimizers\n",
    "from functools import partial\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. MLP y utilidades\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def glorot_init(key, shape):\n",
    "    in_dim, out_dim = shape\n",
    "    limit = jnp.sqrt(6.0 / (in_dim + out_dim))\n",
    "    return random.uniform(key, shape, minval=-limit, maxval=limit)\n",
    "\n",
    "def MLP(layer_sizes, activation=jnp.tanh, out_activation=None):\n",
    "    \"\"\"\n",
    "    Retorna (init_fun, apply_fun) para un MLP fully-connected.\n",
    "    \"\"\"\n",
    "    def init_fun(rng_key):\n",
    "        params = []\n",
    "        keys = random.split(rng_key, len(layer_sizes) - 1)\n",
    "        for k, (m, n) in zip(keys, zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            W = glorot_init(k, (m, n))\n",
    "            b = jnp.zeros((n,))\n",
    "            params.append((W, b))\n",
    "        return params\n",
    "\n",
    "    def apply_fun(params, x):\n",
    "        # x: (..., input_dim)\n",
    "        for i, (W, b) in enumerate(params):\n",
    "            x = jnp.dot(x, W) + b\n",
    "            if i < len(params) - 1:\n",
    "                x = activation(x)\n",
    "            else:\n",
    "                if out_activation is not None:\n",
    "                    x = out_activation(x)\n",
    "        return x\n",
    "\n",
    "    return init_fun, apply_fun\n",
    "\n",
    "def modified_MLP(layer_sizes, activation=jnp.tanh, out_activation=None):\n",
    "    \"\"\"\n",
    "    Versión simple para usar como branch/trunk net.\n",
    "    \"\"\"\n",
    "    return MLP(layer_sizes, activation=activation, out_activation=out_activation)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. DataGenerator (batching)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    Generador de batches al estilo PyTorch, pero simple.\n",
    "    inputs: (U, X)  -> U: (N, m), X: (N, d)\n",
    "    outputs: S: (N, 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, U, X, S, batch_size, shuffle=True):\n",
    "        self.U = U\n",
    "        self.X = X\n",
    "        self.S = S\n",
    "        self.N = U.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.indices = np.arange(self.N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.ptr + self.batch_size > self.N:\n",
    "            # Reiniciamos epoch\n",
    "            self._reset()\n",
    "        idx = self.indices[self.ptr:self.ptr + self.batch_size]\n",
    "        self.ptr += self.batch_size\n",
    "\n",
    "        U_batch = jnp.array(self.U[idx])\n",
    "        X_batch = jnp.array(self.X[idx])\n",
    "        S_batch = jnp.array(self.S[idx])\n",
    "\n",
    "        return (U_batch, X_batch), S_batch\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Modelo: PI_DeepONet para ecuación de onda acústica 2D\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class PI_DeepONet_Acoustic:\n",
    "    \"\"\"\n",
    "    DeepONet físicamente informado para la ecuación de onda acústica 2D:\n",
    "\n",
    "        (1 / c(x,z)^2) * p_tt - (p_xx + p_zz) = 0\n",
    "\n",
    "    Branch net:    u (modelo de velocidad aplanado)   -> B(u)\n",
    "    Trunk net:     (x,z,t)                            -> T(x,z,t)\n",
    "    Operador:      p(x,z,t) = sum_k B_k(u) * T_k(x,z,t)\n",
    "    \"\"\"\n",
    "    def __init__(self, branch_layers, trunk_layers, Nx, Nz,\n",
    "                 lr=1e-3, decay_steps=2000, decay_rate=0.9):\n",
    "        self.Nx = Nx\n",
    "        self.Nz = Nz\n",
    "\n",
    "        # Definición de redes\n",
    "        self.branch_init, self.branch_apply = modified_MLP(branch_layers, activation=jnp.tanh)\n",
    "        self.trunk_init,  self.trunk_apply  = modified_MLP(trunk_layers,  activation=jnp.tanh)\n",
    "\n",
    "        # Inicialización de parámetros\n",
    "        key1, key2 = random.split(random.PRNGKey(1234))\n",
    "        branch_params = self.branch_init(key1)\n",
    "        trunk_params  = self.trunk_init(key2)\n",
    "        params = (branch_params, trunk_params)\n",
    "\n",
    "        # Optimizador Adam con decaimiento exponencial\n",
    "        schedule = optimizers.exponential_decay(lr, decay_steps=decay_steps,\n",
    "                                                decay_rate=decay_rate)\n",
    "        self.opt_init, self.opt_update, self.get_params = optimizers.adam(schedule)\n",
    "        self.opt_state = self.opt_init(params)\n",
    "\n",
    "        self.itercount = itertools.count()\n",
    "\n",
    "        # Logs (opcionales)\n",
    "        self.loss_log   = []\n",
    "        self.loss_res   = []\n",
    "        self.loss_ic_log    = []\n",
    "        self.loss_bc_log    = []\n",
    "        self.loss_data_log  = []\n",
    "\n",
    "    # ------------------------ DeepONet ------------------------\n",
    "\n",
    "    def operator_net(self, params, u, xzt):\n",
    "        \"\"\"\n",
    "        u:   (m,)             modelo de velocidad aplanado\n",
    "        xzt: (3,)             (x, z, t) en índices de grilla\n",
    "        \"\"\"\n",
    "        branch_params, trunk_params = params\n",
    "        B = self.branch_apply(branch_params, u)    # (m_hidden,)\n",
    "        T = self.trunk_apply(trunk_params, xzt)    # (m_hidden,)\n",
    "        return jnp.sum(B * T)                      # escalar p\n",
    "\n",
    "    def get_c_from_u(self, u, xzt):\n",
    "        \"\"\"\n",
    "        Extrae c(x,z) desde u aplanado.\n",
    "        Aquí suponemos que x,z son índices de grilla [0..Nx-1], [0..Nz-1].\n",
    "        \"\"\"\n",
    "        x, z, t = xzt\n",
    "        c_grid = u.reshape(self.Nz, self.Nx)  # (Nz, Nx)\n",
    "\n",
    "        i = jnp.clip(jnp.round(x).astype(jnp.int32), 0, self.Nx - 1)\n",
    "        j = jnp.clip(jnp.round(z).astype(jnp.int32), 0, self.Nz - 1)\n",
    "\n",
    "        return c_grid[j, i]\n",
    "\n",
    "    def residual_net(self, params, u, xzt):\n",
    "        \"\"\"\n",
    "        Residual de la ecuación de onda acústica:\n",
    "            r = (1/c^2) * p_tt - (p_xx + p_zz)\n",
    "        \"\"\"\n",
    "        def net_xzt(X):\n",
    "            return self.operator_net(params, u, X)\n",
    "\n",
    "        # Hessiano completo wrt [x,z,t], X=(x,z,t)\n",
    "        H = hessian(net_xzt)(xzt)    # (3, 3)\n",
    "        p_xx = H[0, 0]\n",
    "        p_zz = H[1, 1]\n",
    "        p_tt = H[2, 2]\n",
    "\n",
    "        c = self.get_c_from_u(u, xzt)\n",
    "        r = (1.0 / (c**2 + 1e-8)) * p_tt - (p_xx + p_zz)\n",
    "        return r\n",
    "\n",
    "    # ------------------------ Predicciones vectorizadas ------------------------\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def predict_p(self, params, U_star, XZT_star):\n",
    "        \"\"\"\n",
    "        U_star:    (N_points, m)\n",
    "        XZT_star:  (N_points, 3)\n",
    "        \"\"\"\n",
    "        return vmap(self.operator_net, (None, 0, 0))(params, U_star, XZT_star)\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def predict_res(self, params, U_star, XZT_star):\n",
    "        return vmap(self.residual_net, (None, 0, 0))(params, U_star, XZT_star)\n",
    "\n",
    "    # ------------------------ Funciones de pérdida ------------------------\n",
    "\n",
    "    def loss_residual(self, params, batch):\n",
    "        (u, xzt), s_target = batch   # s_target ~ 0\n",
    "        r_pred = vmap(self.residual_net, (None, 0, 0))(params, u, xzt)\n",
    "        return jnp.mean(r_pred**2)\n",
    "\n",
    "    def loss_ic(self, params, batch):\n",
    "        (u, xzt), s_target = batch   # p(x,z,0) = 0\n",
    "        p_pred = vmap(self.operator_net, (None, 0, 0))(params, u, xzt)\n",
    "        return jnp.mean((s_target.flatten() - p_pred)**2)\n",
    "\n",
    "    def loss_bc(self, params, batch):\n",
    "        (u, xzt), s_target = batch   # shotgather\n",
    "        p_pred = vmap(self.operator_net, (None, 0, 0))(params, u, xzt)\n",
    "        return jnp.mean((s_target.flatten() - p_pred)**2)\n",
    "\n",
    "    def loss_data(self, params, batch):\n",
    "        (u, xzt), s_target = batch   # snapshots internos\n",
    "        p_pred = vmap(self.operator_net, (None, 0, 0))(params, u, xzt)\n",
    "        return jnp.mean((s_target.flatten() - p_pred)**2)\n",
    "\n",
    "    def total_loss(self, params, res_batch, ic_batch, bc_batch, data_batch,\n",
    "                   w_res=1.0, w_ic=10.0, w_bc=10.0, w_data=10.0):\n",
    "        L_res  = self.loss_residual(params, res_batch)\n",
    "        L_ic   = self.loss_ic(params, ic_batch)\n",
    "        L_bc   = self.loss_bc(params, bc_batch)\n",
    "        L_data = self.loss_data(params, data_batch)\n",
    "\n",
    "        total = w_res*L_res + w_ic*L_ic + w_bc*L_bc + w_data*L_data\n",
    "\n",
    "        return total, (L_res, L_ic, L_bc, L_data)\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def step(self, i, opt_state, res_batch, ic_batch, bc_batch, data_batch,\n",
    "             w_res=1.0, w_ic=10.0, w_bc=10.0, w_data=10.0):\n",
    "        params = self.get_params(opt_state)\n",
    "\n",
    "        def loss_fn(p):\n",
    "            return self.total_loss(p, res_batch, ic_batch, bc_batch, data_batch,\n",
    "                                   w_res, w_ic, w_bc, w_data)[0]\n",
    "\n",
    "        g = grad(loss_fn)(params)\n",
    "        opt_state = self.opt_update(i, g, opt_state)\n",
    "        return opt_state\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Construcción de datasets desde vel_model y wave_field\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_residual_data(vel_flat, Nx, Nz, Nt, N_per_model_res=1000):\n",
    "    \"\"\"\n",
    "    Datos de residual (collocation) para la PDE:\n",
    "        (1/c^2) p_tt - (p_xx + p_zz) = 0\n",
    "    \"\"\"\n",
    "    N_models, m = vel_flat.shape\n",
    "    X_list, U_list, S_list = [], [], []\n",
    "\n",
    "    for n in range(N_models):\n",
    "        u_n = vel_flat[n]  # (m,)\n",
    "\n",
    "        x_idx = np.random.randint(0, Nx, size=N_per_model_res)\n",
    "        z_idx = np.random.randint(0, Nz, size=N_per_model_res)\n",
    "        t_idx = np.random.randint(0, Nt, size=N_per_model_res)\n",
    "\n",
    "        xzt = np.stack([x_idx, z_idx, t_idx], axis=1)   # (N_per_model_res,3)\n",
    "\n",
    "        U_list.append(np.tile(u_n, (N_per_model_res, 1)))  # (N_per_model_res,m)\n",
    "        X_list.append(xzt)\n",
    "        S_list.append(np.zeros((N_per_model_res, 1)))      # residual -> 0\n",
    "\n",
    "    U_res = np.vstack(U_list).astype(np.float32)\n",
    "    X_res = np.vstack(X_list).astype(np.float32)\n",
    "    S_res = np.vstack(S_list).astype(np.float32)\n",
    "    return U_res, X_res, S_res\n",
    "\n",
    "def build_ic_data(vel_flat, Nx, Nz, N_per_model_ic=None):\n",
    "    \"\"\"\n",
    "    Condición inicial: p(x,z,t=0) = 0\n",
    "    \"\"\"\n",
    "    N_models, m = vel_flat.shape\n",
    "    X_list, U_list, S_list = [], [], []\n",
    "\n",
    "    X_grid, Z_grid = np.meshgrid(np.arange(Nx), np.arange(Nz))\n",
    "    x_all = X_grid.flatten()\n",
    "    z_all = Z_grid.flatten()\n",
    "    N_all = x_all.shape[0]\n",
    "\n",
    "    if N_per_model_ic is None or N_per_model_ic > N_all:\n",
    "        N_per_model_ic = N_all\n",
    "\n",
    "    for n in range(N_models):\n",
    "        u_n = vel_flat[n]\n",
    "        idx = np.random.choice(N_all, size=N_per_model_ic, replace=False)\n",
    "        x_idx = x_all[idx]\n",
    "        z_idx = z_all[idx]\n",
    "        t_idx = np.zeros_like(x_idx)  # t=0\n",
    "\n",
    "        xzt = np.stack([x_idx, z_idx, t_idx], axis=1)\n",
    "        U_list.append(np.tile(u_n, (N_per_model_ic, 1)))\n",
    "        X_list.append(xzt)\n",
    "        S_list.append(np.zeros((N_per_model_ic, 1)))  # p=0\n",
    "\n",
    "    U_ic = np.vstack(U_list).astype(np.float32)\n",
    "    X_ic = np.vstack(X_list).astype(np.float32)\n",
    "    S_ic = np.vstack(S_list).astype(np.float32)\n",
    "    return U_ic, X_ic, S_ic\n",
    "\n",
    "def build_bc_shot_data(vel_flat, wave_field, z_shot=2, N_per_model_bc=2000):\n",
    "    \"\"\"\n",
    "    Condición de frontera: shotgather en z = z_shot.\n",
    "\n",
    "    wave_field[n]: (Nt, Nz, Nx)\n",
    "    \"\"\"\n",
    "    N_models, Nt, Nz, Nx = wave_field.shape\n",
    "    X_list, U_list, S_list = [], [], []\n",
    "\n",
    "    # Todos los posibles puntos en esa línea\n",
    "    T_grid, X_grid = np.meshgrid(np.arange(Nt), np.arange(Nx), indexing='ij')\n",
    "    t_all = T_grid.flatten()\n",
    "    x_all = X_grid.flatten()\n",
    "    z_all = np.full_like(x_all, z_shot)\n",
    "    N_all = x_all.shape[0]\n",
    "\n",
    "    if N_per_model_bc > N_all:\n",
    "        N_per_model_bc = N_all\n",
    "\n",
    "    for n in range(N_models):\n",
    "        u_n  = vel_flat[n]\n",
    "        wf_n = wave_field[n]  # (Nt, Nz, Nx)\n",
    "\n",
    "        idx = np.random.choice(N_all, size=N_per_model_bc, replace=False)\n",
    "        t_idx = t_all[idx]\n",
    "        x_idx = x_all[idx]\n",
    "        z_idx = z_all[idx]\n",
    "\n",
    "        # Coordenadas\n",
    "        xzt = np.stack([x_idx, z_idx, t_idx], axis=1)\n",
    "\n",
    "        # Datos observados\n",
    "        s_vals = wf_n[t_idx, z_idx, x_idx][:, None]  # (N_per_model_bc,1)\n",
    "\n",
    "        U_list.append(np.tile(u_n, (N_per_model_bc, 1)))\n",
    "        X_list.append(xzt)\n",
    "        S_list.append(s_vals)\n",
    "\n",
    "    U_bc = np.vstack(U_list).astype(np.float32)\n",
    "    X_bc = np.vstack(X_list).astype(np.float32)\n",
    "    S_bc = np.vstack(S_list).astype(np.float32)\n",
    "    return U_bc, X_bc, S_bc\n",
    "\n",
    "def build_snapshot_data(vel_flat, wave_field, t_indices=[200, 400],\n",
    "                        N_per_model_snap=None):\n",
    "    \"\"\"\n",
    "    Datos internos: snapshots completos en tiempos t_indices.\n",
    "    \"\"\"\n",
    "    N_models, Nt, Nz, Nx = wave_field.shape\n",
    "    X_list, U_list, S_list = [], [], []\n",
    "\n",
    "    X_grid, Z_grid = np.meshgrid(np.arange(Nx), np.arange(Nz), indexing='xy')\n",
    "    x_all = X_grid.flatten()\n",
    "    z_all = Z_grid.flatten()\n",
    "    N_space = x_all.shape[0]\n",
    "\n",
    "    if N_per_model_snap is None or N_per_model_snap > N_space * len(t_indices):\n",
    "        N_per_model_snap = N_space * len(t_indices)\n",
    "\n",
    "    for n in range(N_models):\n",
    "        u_n  = vel_flat[n]\n",
    "        wf_n = wave_field[n]  # (Nt, Nz, Nx)\n",
    "\n",
    "        xzt_list, s_list = [], []\n",
    "        for t_idx in t_indices:\n",
    "            t_vec = np.full_like(x_all, t_idx)\n",
    "            xzt_list.append(np.stack([x_all, z_all, t_vec], axis=1))  # (N_space,3)\n",
    "\n",
    "            s_vals = wf_n[t_idx, :, :].flatten()[:, None]            # (N_space,1)\n",
    "            s_list.append(s_vals)\n",
    "\n",
    "        xzt_all = np.vstack(xzt_list)  # (N_space*len(t_indices),3)\n",
    "        s_all   = np.vstack(s_list)    # (N_space*len(t_indices),1)\n",
    "        N_all   = xzt_all.shape[0]\n",
    "\n",
    "        idx = np.random.choice(N_all, size=N_per_model_snap, replace=False)\n",
    "        xzt = xzt_all[idx]\n",
    "        s_vals = s_all[idx]\n",
    "\n",
    "        U_list.append(np.tile(u_n, (N_per_model_snap, 1)))\n",
    "        X_list.append(xzt)\n",
    "        S_list.append(s_vals)\n",
    "\n",
    "    U_data = np.vstack(U_list).astype(np.float32)\n",
    "    X_data = np.vstack(X_list).astype(np.float32)\n",
    "    S_data = np.vstack(S_list).astype(np.float32)\n",
    "    return U_data, X_data, S_data\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Entrenamiento\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_model(model,\n",
    "                res_dataset,\n",
    "                ic_dataset,\n",
    "                bc_dataset,\n",
    "                data_dataset,\n",
    "                nIter=5000,\n",
    "                w_res=1.0, w_ic=10.0, w_bc=10.0, w_data=10.0):\n",
    "\n",
    "    res_iter  = iter(res_dataset)\n",
    "    ic_iter   = iter(ic_dataset)\n",
    "    bc_iter   = iter(bc_dataset)\n",
    "    data_iter = iter(data_dataset)\n",
    "\n",
    "    pbar = trange(nIter)\n",
    "    for it in pbar:\n",
    "        res_batch  = next(res_iter)\n",
    "        ic_batch   = next(ic_iter)\n",
    "        bc_batch   = next(bc_iter)\n",
    "        data_batch = next(data_iter)\n",
    "\n",
    "        i = next(model.itercount)\n",
    "        model.opt_state = model.step(i, model.opt_state,\n",
    "                                     res_batch, ic_batch, bc_batch, data_batch,\n",
    "                                     w_res, w_ic, w_bc, w_data)\n",
    "\n",
    "        if it % 100 == 0:\n",
    "            params = model.get_params(model.opt_state)\n",
    "            total, (L_res, L_ic, L_bc, L_data) = model.total_loss(\n",
    "                params, res_batch, ic_batch, bc_batch, data_batch,\n",
    "                w_res, w_ic, w_bc, w_data\n",
    "            )\n",
    "            model.loss_log.append(float(total))\n",
    "            model.loss_res.append(float(L_res))\n",
    "            model.loss_ic_log.append(float(L_ic))\n",
    "            model.loss_bc_log.append(float(L_bc))\n",
    "            model.loss_data_log.append(float(L_data))\n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": float(total),\n",
    "                \"L_res\": float(L_res),\n",
    "                \"L_ic\": float(L_ic),\n",
    "                \"L_bc\": float(L_bc),\n",
    "                \"L_data\": float(L_data)\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af333296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta esta ruta a la tuya\n",
    "DATA_PATH = os.path.join(\"..\",\"..\",\"..\", \"propagator\",\"data\")\n",
    "\n",
    "# Carga de datos de ejemplo\n",
    "print(\"Cargando datos...\")\n",
    "vel_model = np.load(os.path.join(DATA_PATH, \"speedwaves\", \"speedwave_00.npy\"))\n",
    "wave_field = np.load(os.path.join(DATA_PATH, \"wavefields\", \"mydataset_ricker\", \"wavefield_00.npy\"))\n",
    "print(\"Done\")\n",
    "\n",
    "# vel_model: (N_models, 1, Nz, Nx)\n",
    "N_models, _, Nz, Nx = vel_model.shape\n",
    "# wave_field: (N_models, Nt, Nz, Nx)\n",
    "Nt = wave_field.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplanar modelos de velocidad\n",
    "print(\"Aplanando datos...\")\n",
    "vel_flat = vel_model[:, 0].reshape(N_models, -1).astype(np.float32)  # (N_models, m)\n",
    "m = vel_flat.shape[1]\n",
    "print(\"Done\")\n",
    "\n",
    "# Construcción de datasets\n",
    "print(\"Construyendo datasets...\")\n",
    "U_res,  X_res,  S_res  = build_residual_data(vel_flat, Nx, Nz, Nt,\n",
    "                                              N_per_model_res=1000)\n",
    "U_ic,   X_ic,   S_ic   = build_ic_data(vel_flat, Nx, Nz,\n",
    "                                        N_per_model_ic=1000)\n",
    "U_bc,   X_bc,   S_bc   = build_bc_shot_data(vel_flat, wave_field,\n",
    "                                            z_shot=2, N_per_model_bc=1000)\n",
    "U_data, X_data, S_data = build_snapshot_data(vel_flat, wave_field,\n",
    "                                              t_indices=[200, 400],\n",
    "                                              N_per_model_snap=1000)\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa7237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataGenerators\n",
    "print(\"Creando DataGenerators...\")\n",
    "batch_size = 8192\n",
    "res_dataset  = DataGenerator(U_res,  X_res,  S_res,  batch_size)\n",
    "ic_dataset   = DataGenerator(U_ic,   X_ic,   S_ic,   batch_size)\n",
    "bc_dataset   = DataGenerator(U_bc,   X_bc,   S_bc,   batch_size)\n",
    "data_dataset = DataGenerator(U_data, X_data, S_data, batch_size)\n",
    "print(\"Done\")\n",
    "\n",
    "# Definición del modelo DeepONet\n",
    "print(\"Creando  el modelo...\")\n",
    "branch_layers = [m, 200, 200, 200, 200]\n",
    "trunk_layers  = [3, 200, 200, 200, 200]\n",
    "\n",
    "model = PI_DeepONet_Acoustic(branch_layers, trunk_layers, Nx=Nx, Nz=Nz,\n",
    "                              lr=1e-3, decay_steps=2000, decay_rate=0.9)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e623df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento (ajusta nIter según quieras)\n",
    "print(\"Entrenando el modelo...\")\n",
    "train_model(model,\n",
    "            res_dataset,\n",
    "            ic_dataset,\n",
    "            bc_dataset,\n",
    "            data_dataset,\n",
    "            nIter=5000,\n",
    "            w_res=1.0, w_ic=10.0, w_bc=10.0, w_data=10.0)\n",
    "print(\"Done\")\n",
    "# Ejemplo de predicción en un modelo específico y puntos dados\n",
    "params = model.get_params(model.opt_state)\n",
    "n_test = 0\n",
    "u_test = jnp.array(vel_flat[n_test])  # (m,)\n",
    "\n",
    "# Puntos (x,z,t) para evaluar\n",
    "x_test = np.linspace(0, Nx-1, 50)\n",
    "z_test = np.linspace(0, Nz-1, 50)\n",
    "t_test = np.array([200], dtype=np.float32)\n",
    "\n",
    "Xg, Zg, Tg = np.meshgrid(x_test, z_test, t_test, indexing=\"ij\")\n",
    "XZT_star = np.stack([Xg.flatten(), Zg.flatten(), Tg.flatten()], axis=1).astype(np.float32)\n",
    "U_star   = np.tile(np.array(vel_flat[n_test], dtype=np.float32), (XZT_star.shape[0], 1))\n",
    "\n",
    "p_pred = model.predict_p(params, jnp.array(U_star), jnp.array(XZT_star))\n",
    "p_pred = np.array(p_pred).reshape(len(x_test), len(z_test), len(t_test))\n",
    "\n",
    "print(\"Predicción p(x,z,t=200) de shape:\", p_pred.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
